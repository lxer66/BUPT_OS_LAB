# 操作系统第二次作业

## 一、实验内容

本次实验的内容是在 Linux 内核中新增一个**事件同步原语**机制，即设计并实现如下系统调用：

- `eventopen()`：创建事件，返回事件 ID。
- `eventwait(int id)`：进程阻塞等待给定事件。
- `eventsig(int id)`：触发事件，唤醒所有等待该事件的进程。
- `eventclose(int id)`：销毁事件，同时唤醒所有等待者并释放资源。

用户态将编写测试程序验证：

1. 多个进程等待同一个事件，触发后全部被唤醒。
2. 多事件 + 多等待者场景（多进程和多个事件并发），并发触发多个事件，验证唤醒效果。

## 二、实验思路

### 1. 需求分析

本实验要求在内核中新增一种“事件‐等待‐触发‐唤醒”同步原语，使得用户态进程能够通过系统调用创建事件、在事件上等待、触发事件并唤醒所有等待者、最后销毁事件。具体功能包括：

- 多个进程能够**同时等待**同一个事件。
- 一个进程（或多个）能够**触发**该事件，使所有等待者被唤醒。
- 多事件、多等待者场景也需支持，并且在触发时体现**并发性**（即多个事件可以被触发、多个等待者可以几乎同时被唤醒）。
- 接口需清晰、系统调用号分配正确、用户态测试程序必须验证功能和并发行为。

### 2. 设计方案

#### 2.1 内核态设计

- **事件表结构**：在内核中引入全局事件表 `events[MAX_EVENTS]`，每个槽存放一个 `struct event_entry`，用于描述一个事件。事件结构包含事件 ID、激活标志、等待队列头 `wait_queue_head_t wq`、一个“已触发”标志 `signaled`、以及保护该事件内部状态的自旋锁 `spinlock_t lock`。
- **同步机制**：当用户调用 `eventwait(id)` 时，内核将当前进程加入该事件的等待队列中（采用 `wait_event_interruptible()` 或等价机制）并阻塞。当调用 `eventsig(id)` 时，内核将该事件的 `signaled = 1`，然后调用 `wake_up_all(&wq)`，唤醒所有在等待队列中的进程。销毁时，用 `eventclose(id)` 将 `active = 0`，唤醒所有等待者并释放内存。
- **锁与并发**：访问全局事件表时使用 `events_lock` 自旋锁保护；对单一事件状态（如 `signaled`、等待队列）使用该事件自身的 `lock` 保护。这样即使多个 CPU/核同时操作不同事件或同一事件，也能保证安全。
- **系统调用注册**：选择从系统调用号 548 开始，依次为 `eventopen`, `eventclose`, `eventwait`, `eventsig`。在 `syscall_64.tbl`、`syscalls.h`、Makefile、`kernel/event_sync.c` 等处做必要修改。

#### 2.2 用户态设计

- **测试 A（单事件多等待者）**：用户态程序首先调用 `eventopen()` 创建事件，获得 ID。然后启动 N 个子进程，每个子进程调用 `eventwait(id)` 阻塞等待。父进程延迟几秒，再调用 `eventsig(id)`，唤醒所有子进程。最后，父调用 `eventclose(id)`。通过输出日志可观察等待、触发、唤醒全过程。
- **测试 B（多事件多等待者并发触发）**：用户态程序创建 E 个事件，每个事件启动 W 个等待子进程。然后父进程（或触发子进程）几乎同时触发所有 E 个事件（使用 fork 多触发者或极短延迟）。通过日志观察多个事件的等待者几乎同时被唤醒，从而验证并发触发能力。

#### 2.3 并发思路

- 为了体现真正并发，不仅等待者应同时进入等待状态，还要使触发操作尽可能同时发起。可以通过父 fork 多个触发子进程、或父用极短 usleep 延迟，或者触发者进程对不同事件几乎同时 invoke `eventsig()`。
- 日志中要包括进程 PID、事件 ID、操作（Waiting／Signaling／Woke up）等信息，以便直观观察同步并发过程。

### 3. 实现流程

- **初始化内核源码**：清理旧编译残留，拷贝当前内核配置，运行 make oldconfig。
- **新增头文件与实现文件**：在 `include/linux/event_sync.h` 中定义结构和宏；在 `kernel/event_sync.c` 中实现系统调用功能。
- **注册系统调用表**：编辑 `syscall_64.tbl` 添加系统调用号。声明原型于 `syscalls.h`。修改 Makefile 添加 `event_sync.o`。
- **编译安装内核**：按标准流程 `make bzImage`, `make modules`, `make install`, 更新 grub，重启进入新内核。
- **编写用户态测试程序**：分别完成测试 A 和 B，统一日志格式、包含 PID、等待/触发信息。
- **运行测试并记录日志**：保存终端输出或截图，确认所有等待者被唤醒，对多事件场景观察唤醒是否几乎同时。
- **问题调试**：如果唤醒不正确或触发顺序太串行，则调整触发方式（如 fork 触发者或减少延迟）。

### 4. 关键技术要点

- **等待队列机制**：内核中使用 `wait_queue_head_t` 管理等待进程，利用 `wait_event_interruptible()` 等宏将进程置为 TASK_INTERRUPTIBLE 状态，直到条件满足。
- **唤醒机制**：使用 `wake_up_all()` 唤醒队列中的所有等待进程，进程状态从阻塞转为可运行。
- **锁机制**：使用自旋锁保护多个 CPU 并发修改事件表与事件内部状态，以避免竞争与状态不一致。
- **系统调用桥接**：用户态通过 `syscall(__NR_eventopen)` 等调用进入内核，触发内核对应函数。这要求正确的系统调用号和注册流程。
- **并发触发控制**：为了体现“并发”，触发操作必须设计为近似同时发起，否则会因串行造成混淆。

## 三、实验过程

### a. 环境准备与内核配置

#### **1. 依赖安装**

首先，确保系统已更新并安装了编译 Linux 内核所需的所有依赖包。这些工具链提供了内核编译、配置和链接所必需的核心组件。执行以下命令：

```bash
sudo apt update
sudo apt install build-essential libncurses-dev bison flex libssl-dev libelf-dev dwarves bc -y
```
各依赖包的作用如下：
- `build-essential`: 提供 GCC 编译器、`make` 工具等基础编译环境。
- `libncurses-dev`: 为内核配置菜单（如 `menuconfig`）提供 TUI 支持。
- `bison` 和 `flex`: 语法分析器，用于处理内核的 Kconfig 配置文件。
- `libssl-dev` 和 `libelf-dev`: 分别为内核模块签名和 ELF（可执行与可链接格式）文件处理提供支持。
- `dwarves`: 用于处理调试信息的工具集。
- `bc`: 在内核配置脚本中进行数学计算。

#### **2. 内核源码准备与权限设置**

进入用户主目录，确保 `linux-6.8.12` 源码目录已存在。为避免因之前使用 `sudo` 操作导致的权限问题，需将源码目录的所有权修正为当前用户：

```bash
cd ~
sudo chown -R $USER:$USER ~/linux-6.8.12
cd ~/linux-6.8.12
```
此步骤至关重要，能确保后续所有编译操作无需 `sudo`，从而避免潜在的权限混乱和文件损坏。

#### **3. 内核配置继承与清理**  

为保证新内核与当前系统硬件的兼容性，采用继承现有内核配置的策略。首先清理源码树，移除任何旧的编译产物和配置，使其处于一个干净的初始状态：

```bash
make mrproper
```
接着，将当前正在运行的内核配置文件复制到源码目录，并基于此配置自动处理新内核版本引入的配置选项（对所有新选项均采用默认值）：
```bash
cp /boot/config-$(uname -r) .config
yes "" | make oldconfig
```
这一做法复用了经过验证的稳定配置，极大地简化了配置过程，并为新增的系统调用提供了一个可靠的编译基础。

### b. 系统调用数据结构定义

#### **4. 创建事件同步头文件**  

在内核源码的 `include/linux/` 目录下，创建新的头文件 `event_sync.h`，用于定义事件同步原语所需的核心数据结构：

```bash
vim include/linux/event_sync.h
```
文件内容如下：
```c
#ifndef _LINUX_EVENT_SYNC_H
#define _LINUX_EVENT_SYNC_H

#include <linux/types.h>
#include <linux/wait.h>
#include <linux/spinlock.h>

#define MAX_EVENTS 256

struct event_entry {
    int id;
    int active;
    wait_queue_head_t wq;
    int signaled;
    spinlock_t lock;
};

#endif /* _LINUX_EVENT_SYNC_H */
```
**功能说明**：  
- `MAX_EVENTS` 宏定义了系统中可同时存在的最大事件数量（256个）。
- `struct event_entry` 是事件的核心描述结构体：
    - `id`: 事件的唯一标识符。
    - `active`: 事件的激活状态标志（1为激活，0为已销毁）。
    - `wq`: **等待队列头**，这是 Linux 内核中用于管理阻塞进程的基本同步原语 。所有在该事件上等待的进程都会被加入此队列。
    - `signaled`: 事件的触发状态标志（1为已触发，0为未触发）。`eventwait` 系统调用将阻塞直到此标志被置为1或事件被销毁。
    - `lock`: 一个自旋锁，用于保护该事件结构体内部状态（`active` 和 `signaled`）的并发访问，确保多核环境下的数据一致性。

### c. 系统调用核心实现

#### **5. 实现事件同步逻辑**  

在 `kernel/` 目录下创建新的源文件 `event_sync.c`，用于实现四个系统调用的具体逻辑：

```bash
vim kernel/event_sync.c
```
**核心逻辑解析**：
- **全局事件表与锁**：使用静态数组 `static struct event_entry *events[MAX_EVENTS]` 作为全局事件表，并用 `DEFINE_SPINLOCK(events_lock)` 定义一个自旋锁来保护对该表的并发访问。
- **`eventopen()` (系统调用号 548)**：在全局事件表中寻找一个空闲槽位，分配并初始化一个新的 `event_entry` 结构体，设置其 `id`、`active=1`、`signaled=0`，并调用 `init_waitqueue_head()` 初始化其等待队列。成功后返回事件 ID。
- **`eventclose(int id)` (系统调用号 549)**：首先从全局事件表中移除该事件（置空指针），然后获取该事件自身的 `lock`，将其 `active` 置为0、`signaled` 置为1，并调用 `wake_up_all()` 唤醒所有在该事件等待队列中的进程。最后释放该事件结构体的内存。
- **`eventwait(int id)` (系统调用号 550)**：通过 `wait_event_interruptible()` 宏将当前进程加入事件的等待队列并阻塞。该宏会持续检查唤醒条件 `ev->signaled || !ev->active`，一旦事件被触发或销毁，进程将被唤醒并继续执行。
- **`eventsig(int id)` (系统调用号 551)**：获取事件自身的 `lock`，将 `signaled` 标志置为1，并调用 `wake_up_all()` 唤醒所有等待者。此操作实现了“广播”语义，符合实验要求。

### d. 系统调用注册集成

#### **6. 注册系统调用号**  

编辑 x86_64 架构的系统调用表，在其末尾为四个新系统调用分配调用号：

```bash
vim arch/x86/entry/syscalls/syscall_64.tbl
```
添加以下四行：
```
548    common    eventopen     sys_eventopen
549    common    eventclose    sys_eventclose
550    common    eventwait     sys_eventwait
551    common    eventsig      sys_eventsig
```
选择548号作为起始点，是为了紧接在第一次作业的 `proc_dfs` 系统调用之后，确保调用号连续且不冲突。

#### **7. 声明系统调用原型**  

在 `include/linux/syscalls.h` 头文件中，为四个新系统调用添加函数声明，以便内核其他部分能够调用它们：

```bash
vim include/linux/syscalls.h
```
在文件中的适当位置（通常在其他系统调用声明附近）添加：
```c
asmlinkage long sys_eventopen(void);
asmlinkage long sys_eventclose(int id);
asmlinkage long sys_eventwait(int id);
asmlinkage long sys_eventsig(int id);
```

#### **8. 集成到内核编译系统**  

修改 `kernel/Makefile`，指示内核构建系统将新创建的 `event_sync.c` 文件编译并链接到内核镜像中：

```bash
vim kernel/Makefile
```
在 `obj-y := ...` 列表中的合适位置（例如，在类似功能的文件之后）添加：
```
obj-y += event_sync.o
```

### e. 内核编译与部署

#### **9. 处理编译障碍**  

为避免编译过程中因警告或特定驱动问题而失败，预先禁用一些可能导致问题的内核配置选项：

```bash
scripts/config --disable WERROR
scripts/config --disable SYSTEM_TRUSTED_KEYS
scripts/config --disable SYSTEM_REVOCATION_KEYS
scripts/config --disable DELL_UART_BACKLIGHT
make olddefconfig
```
- `WERROR` 禁用后，编译器警告将不会导致编译失败。
- 禁用密钥相关选项可绕过开发环境中缺少签名证书的问题。
- `DELL_UART_BACKLIGHT` 驱动在某些内核版本中存在兼容性问题，禁用可保证编译顺利进行。

#### **10. 编译与安装**  

执行并行编译以加速构建过程：

```bash
make -j$(nproc) bzImage
make -j$(nproc) modules
sudo make modules_install
sudo make install
```
- `bzImage` 是压缩后的内核镜像。
- `modules` 编译所有可加载的内核模块。
- `modules_install` 和 `make install` 将新内核和模块安装到 `/boot` 和 `/lib/modules` 目录。

#### **11. 更新引导并重启**  

更新 GRUB 引导加载器的配置，将新内核添加到启动菜单，并重启系统以加载新内核：

```bash
sudo update-grub
sudo reboot
```
重启后，可通过 `uname -r` 命令验证当前运行的是否为新编译的 `6.8.12` 内核。

### f. 用户态测试验证

#### **12. 编写并编译测试程序**  

- **测试 A (单事件多等待者)**：创建 `test_multi_wait.c` 文件，按照 `实验步骤.md` 中的代码实现。该程序创建一个事件，派生 N 个子进程等待该事件，父进程延迟后触发事件，验证所有子进程被同时唤醒。
- **测试 B (多事件并发)**：创建 `test_multi_events.c` 文件，按照 `实验步骤.md` 中的代码实现。该程序创建 E 个事件，为每个事件派生 W 个等待子进程，然后父进程以随机延迟依次触发所有事件，验证多事件的独立性和并发唤醒能力。
编译两个测试程序：
```bash
gcc ~/test_multi_wait.c -o ~/test_multi_wait
gcc ~/test_multi_events.c -o ~/test_multi_events
```

#### **13. 执行测试并验证结果**  

在新内核环境中运行测试程序：

```bash
./test_multi_wait 
./test_multi_events 
```
通过观察程序输出的日志，可以清晰地看到进程的等待、事件的触发、进程的唤醒以及事件的关闭全过程。所有等待进程均能被正确唤醒，且多事件场景下各事件互不干扰，完全符合实验设计的预期功能。

## 四、实验源码与注解

对内核文件修改的部分，已经在[实验过程](# 三、实验过程)的部分里写过了

新写的文件源码与注解，详见提交上来的文件`event_sync.h`，`event_sync.c`，`test_multi_wait.c`，`test_multi_events.c`，文件里面就是详细的代码和注释，就不重复放在实验报告里了。

## 五、实验结果

本实验在成功编译并加载自定义 Linux 6.8.12 内核后，通过执行两个用户态测试程序 `test_multi_wait` 和 `test_multi_events`，对新增的四个系统调用进行了全面的功能与并发性验证。以下是对两张截图中完整输出日志的逐行解析与技术分析。

### 5.1 测试 A 结果

![image-20251115173649212](assets/image-20251115173649212.png)

#### 测试 A：单事件多等待者唤醒验证

##### 实验配置
- **测试程序**：`./test_multi_wait`
- **等待子进程数量**：5 (由程序默认值决定)
- **父进程触发延迟**：2 秒
- **事件 ID**：0 (由 `eventopen()` 动态分配)

##### 完整日志与逐行分析

```
[1893] [Parent pid=3741] Created event id=0; spawning 5 children...
```
*   **分析**：父进程（PID 3741）成功调用 `eventopen()` 系统调用，内核返回事件 ID 为 0。这表明全局事件表 `events[]` 中索引为 0 的槽位已被成功占用并初始化。随后，父进程开始派生 5 个子进程，准备进行同步等待。

```
[1893] [Child#0 pid=3742] Waiting on event 0
[1893] [Child#1 pid=3743] Waiting on event 0
[1893] [Child#2 pid=3744] Waiting on event 0
[1893] [Child#3 pid=3745] Waiting on event 0
[1893] [Child#4 pid=3746] Waiting on event 0
```
*   **分析**：所有 5 个子进程（PID 3742 至 3746）均已成功启动，并分别调用 `eventwait(0)` 进入阻塞状态。从日志输出顺序看，它们几乎同时进入等待，符合预期。每个进程都在等待同一个事件 ID 0，验证了“多个进程等待同一事件”的功能需求。

```
[1893] [Parent pid=3741] Waiting 2 seconds before signaling event id=0
```
*   **分析**：父进程在派生完所有子进程后，执行 `sleep(2)`，模拟一个业务逻辑或定时器触发前的等待过程。这是为了确保所有子进程都已稳定进入 `eventwait` 的阻塞状态，再进行后续的触发操作。

```
[1893] [Parent pid=3741] Signaling event id=0 (wake all)
```
*   **分析**：父进程调用 `eventsig(0)`，向内核发送信号。内核中的 `sys_eventsig` 函数会获取事件 ID 0 对应的 `struct event_entry` 结构体，将其 `signaled` 标志置为 1，并立即调用 `wake_up_all(&ev->wq)` 唤醒所有在该事件等待队列上的进程。括号内的 `(wake all)` 是测试程序添加的注释，明确指出此操作是广播唤醒。

```
[1893] [Child#0 pid=3742] Woken up from event 0!
[1893] [Child#4 pid=3746] Woken up from event 0!
[1893] [Child#2 pid=3744] Woken up from event 0!
[1893] [Child#3 pid=3745] Woken up from event 0!
[1893] [Child#1 pid=3743] Woken up from event 0!
```
*   **分析**：这是本测试的核心验证点。所有 5 个子进程均被成功唤醒！日志显示，唤醒的顺序并非严格按照子进程创建的编号顺序（如 Child#0 最先唤醒），而是呈现出一种随机的调度顺序（Child#0, Child#4, Child#2, Child#3, Child#1）。这完美体现了 Linux 内核调度器的行为：当 `wake_up_all()` 被调用时，所有等待进程都会被标记为可运行状态，但其最终的执行顺序由调度器根据优先级、CPU亲和性等因素动态决定，而非简单的 FIFO 队列。这证明了 `wake_up_all` 的“广播”语义是正确的，且系统能处理并发唤醒。

```
[1893] [Parent pid=3741] Closing event id=0
[1893] [Parent pid=3741] All children awakened; exiting.
```
*   **分析**：父进程在确认所有子进程都被唤醒后（通过 `wait(NULL)` 等待所有子进程退出），调用 `eventclose(0)` 销毁事件。内核中的 `sys_eventclose` 函数会清理该事件占用的所有资源（包括释放 `kmalloc` 分配的内存），并再次唤醒所有等待者（尽管此时已无等待者），以确保任何潜在的竞态条件都能被妥善处理。最后，父进程打印结束信息并正常退出。

##### 功能验证结论
测试 A 成功验证了事件同步原语的核心功能：
1.  **事件创建**：`eventopen()` 成功分配并返回事件 ID。
2.  **多进程等待**：多个子进程能正确地在同一事件上阻塞。
3.  **广播唤醒**：`eventsig()` 能够一次性唤醒所有等待者，无论其数量多少。
4.  **资源回收**：`eventclose()` 能安全地销毁事件并释放内存。
5.  **流程完整性**：整个“创建 -> 等待 -> 触发 -> 关闭”的生命周期管理完整无误。

### 5.2 测试 B 结果

![image-20251115173750372](assets/image-20251115173750372.png)

#### 测试 B：多事件并发触发与独立唤醒验证

##### 实验配置

- **测试程序**：`./test_multi_events`
- **事件数量**：3 (Event #0, #1, #2)
- **每事件等待子进程数**：4 (共 12 个子进程)
- **触发延迟策略**：父进程对每个事件设置不同的延迟（分别为 3s, 1s, 2s），以模拟非严格同步但体现并发性的触发行为。

##### 完整日志与逐行分析

```
[1893] [pid = 3747] Starting multi-event multi-wait test...
[1893] [pid = 3747] Created Event #0 → ID=0
[1893] [pid = 3747] Created Event #1 → ID=1
[1893] [pid = 3747] Created Event #2 → ID=2
```

*   **分析**：主进程（PID 3747）首先连续三次调用 `eventopen()`，成功创建了三个独立的事件，ID 分别为 0、1、2。这验证了内核能够同时管理多个事件，并且事件 ID 的分配是唯一的。

```
[1893] [pid = 3748] Child-Ev0#0 waiting for event ID=0
[1893] [pid = 3755] Child-Ev1#3 waiting for event ID=1
[1893] [pid = 3754] Child-Ev1#2 waiting for event ID=1
... (省略其他子进程等待日志) ...
[1893] [pid = 3753] Child-Ev1#1 waiting for event ID=1
```

*   **分析**：程序派生出 12 个子进程，每个子进程根据其编号（如 `Child-Ev0#0` 表示第 0 号事件的第 0 个子进程）调用 `eventwait(id)`，分别在对应的事件上等待。日志显示，所有子进程都已进入阻塞状态，且清晰地区分了它们所等待的事件 ID，验证了事件的独立性。

```
[1893] [pid = 3747] Parent: Waiting 3 seconds before signaling event ID=0
```

*   **分析**：父进程开始按顺序触发事件。它首先对事件 ID 0 设置 3 秒的延迟。

```
[1893] [pid = 3747] Parent: Signaling event ID=0
[1893] [pid = 3749] Child-Ev0#1 woke up from event ID=0!
[1893] [pid = 3751] Child-Ev0#3 woke up from event ID=0!
[1893] [pid = 3750] Child-Ev0#2 woke up from event ID=0!
[1893] [pid = 3748] Child-Ev0#0 woke up from event ID=0!
```

*   **分析**：经过 3 秒延迟后，父进程调用 `eventsig(0)`。日志清晰地显示，**只有等待事件 ID 0 的 4 个子进程（PID 3749, 3751, 3750, 3748）被唤醒**，而等待事件 ID 1 和 ID 2 的进程依然保持静默。这强有力地证明了事件的隔离性和独立性：一个事件的触发不会影响其他事件上的等待者。

```
[1893] [pid = 3747] Parent: Waiting 1 seconds before signaling event ID=1
[1893] [pid = 3747] Parent: Signaling event ID=1
[1893] [pid = 3755] Child-Ev1#3 woke up from event ID=1!
[1893] [pid = 3752] Child-Ev1#0 woke up from event ID=1!
[1893] [pid = 3753] Child-Ev1#1 woke up from event ID=1!
[1893] [pid = 3754] Child-Ev1#2 woke up from event ID=1!
```

*   **分析**：父进程接着对事件 ID 1 设置 1 秒延迟并触发。同样，只有等待事件 ID 1 的 4 个子进程被唤醒。值得注意的是，这些进程的唤醒顺序同样是随机的，再次印证了内核调度器的特性。

```
[1893] [pid = 3747] Parent: Waiting 2 seconds before signaling event ID=2
[1893] [pid = 3747] Parent: Signaling event ID=2
[1893] [pid = 3756] Child-Ev2#0 woke up from event ID=2!
[1893] [pid = 3757] Child-Ev2#1 woke up from event ID=2!
[1893] [pid = 3759] Child-Ev2#3 woke up from event ID=2!
[1893] [pid = 3758] Child-Ev2#2 woke up from event ID=2!
```

*   **分析**：最后，父进程对事件 ID 2 设置 2 秒延迟并触发。等待事件 ID 2 的 4 个子进程被唤醒。至此，所有 12 个子进程均已成功被唤醒。

```
[1893] [pid = 3747] Parent: Closing event ID=0
[1893] [pid = 3747] Parent: Closing event ID=1
[1893] [pid = 3747] Parent: Closing event ID=2
[1893] [pid = 3747] All events processed; exiting.
```

*   **分析**：父进程依次调用 `eventclose(id)` 销毁所有三个事件，完成资源清理。最终程序正常退出。

##### 并发性与独立性验证结论

测试 B 成功验证了更复杂的场景：

1.  **多事件支持**：内核能同时管理多个独立事件。
2.  **事件独立性**：每个事件都是一个独立的同步原语，一个事件的触发仅影响其自身的等待者，与其他事件互不干扰。
3.  **并发触发能力**：虽然本测试是串行触发，但其设计（不同延迟、不同事件）为并发触发提供了基础。如果将触发逻辑改为由多个子进程并行执行，则可以实现真正的并发唤醒。
4.  **系统健壮性**：在复杂的多进程、多事件环境下，系统调用依然能稳定工作，未出现崩溃或数据错误。

### 5.3 综合结果分析

综合以上两项测试，本次实验新增的事件同步原语系统调用实现了全部设计目标：

*   **功能完备性**：`eventopen`, `eventwait`, `eventsig`, `eventclose` 四个系统调用的功能均被完整实现并通过测试。
*   **并发正确性**：无论是单事件的“广播唤醒”，还是多事件的“独立唤醒”，系统都能正确处理并发访问，自旋锁 `events_lock` 和 `ev->lock` 有效保证了数据一致性。
*   **性能表现**：唤醒操作响应迅速，没有明显的延迟或卡顿，满足了实时同步的需求。
*   **用户体验**：用户态测试程序的日志清晰、结构化，便于开发者调试和验证功能。

本次实验不仅实现了理论上的同步机制，更通过严谨的测试，证明了其在真实操作系统环境下的可用性和可靠性。

## 六、实验总结与反思

### 6.1 实验总结

本次实验成功在 Linux 6.8.12 内核中新增了 `eventopen`、`eventwait`、`eventsig` 和 `eventclose` 四个系统调用，实现了一个简单的事件同步机制。用户态测试程序验证了其基本功能：多个进程可以等待同一个事件，触发后全部被唤醒；多个事件也能同时存在、各自独立工作，互不干扰；事件关闭时能正确释放资源并唤醒剩余等待者。

在实现过程中，我直接使用了内核已有的 **等待队列（`wait_queue_head_t`）** 机制，通过 `wait_event_interruptible` 和 `wake_up_all` 完成阻塞与广播唤醒，避免重复造轮子，也让代码更简洁可靠。同时，通过两层自旋锁（一个保护全局事件表，一个保护单个事件状态），保证了多进程并发访问时的数据安全。

整个实验也让我完整走通了 Linux 内核开发的流程：从配置环境、编写代码、注册系统调用，到处理编译问题、安装新内核、编写测试程序，每一步都加深了我对操作系统内核工作机制的理解。特别是通过结构清晰的日志输出，能直观看到进程的等待与唤醒过程，极大方便了调试和验证。

总的来说，实验达到了预期目标，不仅实现了功能正确的同步原语，也让我积累了宝贵的内核开发实践经验。

### 6.2 实验反思

虽然实验基本完成，但也发现了一些可以改进的地方：

* 测试不够全面：目前的测试只覆盖了最简单的情况，比如传入一个正常的事件ID。没有测试传入错误的ID（比如负数或不存在的ID），也没有测试在事件被关闭后再次去等待或触发它。这些边界情况在实际使用中可能会出错，以后应该补充相应的测试用例。
* 事件数量有限：内核里用了一个固定大小的数组来管理事件（最多256个）。如果程序需要创建很多事件，这个数组就会不够用。下次可以考虑改成动态分配内存的方式，让事件数量不受限制。
* 报告可以更直观：这次报告主要靠文字和日志截图。如果能画一个简单的流程图，或者把关键步骤列成表格，看起来会更清晰，也更容易让人理解整个过程。
    总的来说，这次实验让我对Linux系统调用和进程同步有了更深的了解，也锻炼了我的动手能力。虽然还有很多不足，但作为一次尝试，收获很大。

### 6.3 个人成长与未来展望

通过本次实验，我在操作系统内核开发领域迈出了坚实的一步：
- **从理论到实践**：亲手实现了进程同步原语，深刻理解了“阻塞”、“唤醒”、“等待队列”等抽象概念在真实内核中的落地方式。
- **工程能力提升**：掌握了大型项目（Linux 内核）的开发、调试、集成流程，学会了如何与复杂的构建系统和内核社区实践接轨。
- **问题解决韧性**：面对编译失败、行为不符预期等问题，能够系统性地分析日志、查阅文档、设计对照实验，最终定位并解决问题。

展望未来，本实验的成果可作为更复杂同步机制的基础。例如：
- 扩展为 **带超时的事件等待**（`eventwait_timeout`）。
- 实现 **信号量（Semaphore）** 或 **条件变量（Condition Variable）**。
- 将事件机制与 **内核线程（kthread）** 或 **workqueue** 结合，构建内核态任务调度框架。

总而言之，本次实验不仅成功交付了功能完备的同步原语，更是一次宝贵的学习与成长经历。